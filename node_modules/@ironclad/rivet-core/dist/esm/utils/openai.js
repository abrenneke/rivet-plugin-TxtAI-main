import fetchEventSource from './fetchEventSource.js';
export const openaiModels = {
    'gpt-4': {
        maxTokens: 8192,
        tiktokenModel: 'gpt-4',
        cost: {
            prompt: 0.03,
            completion: 0.06,
        },
        displayName: 'GPT-4',
    },
    'gpt-4-32k': {
        maxTokens: 32768,
        tiktokenModel: 'gpt-4-32k',
        cost: {
            prompt: 0.06,
            completion: 0.12,
        },
        displayName: 'GPT-4 32k',
    },
    'gpt-4-0613': {
        maxTokens: 8192,
        tiktokenModel: 'gpt-4',
        cost: {
            prompt: 0.03,
            completion: 0.06,
        },
        displayName: 'GPT-4 (v0613)',
    },
    'gpt-4-32k-0613': {
        maxTokens: 32768,
        tiktokenModel: 'gpt-4',
        cost: {
            prompt: 0.06,
            completion: 0.12,
        },
        displayName: 'GPT-4 32k (v0613)',
    },
    'gpt-3.5-turbo': {
        maxTokens: 4096,
        tiktokenModel: 'gpt-3.5-turbo',
        cost: {
            prompt: 0.002,
            completion: 0.002,
        },
        displayName: 'GPT-3.5 Turbo',
    },
    'gpt-3.5-turbo-0613': {
        maxTokens: 16384,
        tiktokenModel: 'gpt-3.5-turbo',
        cost: {
            prompt: 0.002,
            completion: 0.002,
        },
        displayName: 'GPT-3.5 (v0613)',
    },
    'gpt-3.5-turbo-16k-0613': {
        maxTokens: 16384,
        tiktokenModel: 'gpt-3.5-turbo',
        cost: {
            prompt: 0.003,
            completion: 0.004,
        },
        displayName: 'GPT-3.5 16k (v0613)',
    },
};
export const openAiModelOptions = Object.entries(openaiModels).map(([id, { displayName }]) => ({
    value: id,
    label: displayName,
}));
export class OpenAIError extends Error {
    status;
    responseJson;
    constructor(status, responseJson) {
        super(`OpenAIError: ${status} ${JSON.stringify(responseJson)}`);
        this.status = status;
        this.responseJson = responseJson;
        this.name = 'OpenAIError';
    }
}
export const DEFAULT_CHAT_ENDPOINT = 'https://api.openai.com/v1/chat/completions';
export async function* streamChatCompletions({ endpoint, auth, signal, headers, ...rest }) {
    const abortSignal = signal ?? new AbortController().signal;
    // Turn off timeout because local models can be slow, TODO configurable timeout
    const timeout = endpoint === DEFAULT_CHAT_ENDPOINT ? 5000 : 10000000;
    const response = await fetchEventSource(endpoint, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            Authorization: `Bearer ${auth.apiKey}`,
            ...(auth.organization ? { 'OpenAI-Organization': auth.organization } : {}),
            ...headers,
        },
        body: JSON.stringify({
            ...rest,
            stream: true,
        }),
        signal: abortSignal,
    }, timeout);
    let hadChunks = false;
    for await (const chunk of response.events()) {
        hadChunks = true;
        if (chunk === '[DONE]' || abortSignal?.aborted) {
            return;
        }
        let data;
        try {
            data = JSON.parse(chunk);
        }
        catch (err) {
            console.error('JSON parse failed on chunk: ', chunk);
            throw err;
        }
        yield data;
    }
    if (!hadChunks) {
        const responseJson = await response.json();
        throw new OpenAIError(response.status, responseJson);
    }
}
