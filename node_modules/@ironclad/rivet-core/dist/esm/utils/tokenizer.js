import { encoding_for_model } from '@dqbd/tiktoken';
import { openaiModels } from './openai.js';
export const supportedModels = [...Object.keys(openaiModels)];
export function getTokenCountForString(input, model) {
    const encoding = encoding_for_model(model);
    const encoded = encoding.encode(input);
    encoding.free();
    return encoded.length;
}
export function getTokenCountForMessages(messages, model) {
    const encoding = encoding_for_model(model);
    const tokenCount = messages.reduce((sum, message) => {
        const encoded = encoding.encode(JSON.stringify(message));
        return sum + encoded.length;
    }, 0);
    encoding.free();
    return tokenCount;
}
export function chunkStringByTokenCount(input, targetTokenCount, model, overlapPercent) {
    overlapPercent = Number.isNaN(overlapPercent) ? 0 : Math.max(0, Math.min(1, overlapPercent));
    const chunks = [];
    const guess = Math.floor(targetTokenCount * (input.length / getTokenCountForString(input, model)));
    let remaining = input;
    while (remaining.length > 0) {
        chunks.push(remaining.slice(0, guess));
        remaining = remaining.slice(guess - Math.floor(guess * overlapPercent));
    }
    return chunks;
}
export function getCostForTokens(tokenCount, type, model) {
    const costPerThousand = openaiModels[model].cost[type];
    return (tokenCount / 1000) * costPerThousand;
}
export function getCostForPrompt(messages, model) {
    const tokenCount = getTokenCountForMessages(messages, openaiModels[model].tiktokenModel);
    return getCostForTokens(tokenCount, 'prompt', model);
}
